{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QL5C28anXx_IAsYNkdQ4GkuVvL29PUZf",
      "authorship_tag": "ABX9TyNWZuNCIQ1XjVchU7QRCXrI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SameerTheProgrammer/ML-financial-data/blob/main/screener_web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_neyRTpA1mR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "# it is used make a folder where i save all data\n",
        "os.mkdir(\"/content/drive/MyDrive/Screener_Comapany_Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps to fetch links of all comanpy and store in list"
      ],
      "metadata": {
        "id": "JypJN4Bv9KXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using this way you can login to that website and fetch data\n",
        "\n",
        "cookies = {\n",
        "    'csrftoken': 'G7OthSSwzawWdx0L4I6p8IMaaHCBPw7r',\n",
        "    'sessionid': '9dchbljukn6ogjc4bu2utbpsgj4k5ybf',\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'authority': 'www.screener.in',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'cache-control': 'max-age=0',\n",
        "    # 'cookie': 'csrftoken=G7OthSSwzawWdx0L4I6p8IMaaHCBPw7r; sessionid=9dchbljukn6ogjc4bu2utbpsgj4k5ybf',\n",
        "    'referer': 'https://www.screener.in/screen/raw/?sort=&order=&source=&query=Market+capitalization+%3E++1&page=1',\n",
        "    'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
        "    'sec-ch-ua-mobile': '?1',\n",
        "    'sec-ch-ua-platform': '\"Android\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',\n",
        "}\n",
        "\n",
        "params = {\n",
        "    'sort': '',\n",
        "    'order': '',\n",
        "    'source': '',\n",
        "    'query': 'Market capitalization >  1',\n",
        "    'limit': '50',\n",
        "    'page': '1',\n",
        "}"
      ],
      "metadata": {
        "id": "jnyut2jl9E4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_referer = 'https://www.screener.in/screen/raw/?sort=&order=&source=&query=Market+capitalization+%3E++1&page='\n",
        "links = []"
      ],
      "metadata": {
        "id": "6vpTco-j9P1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page_number in range(1, 91):  # Loop from page 1 to 91\n",
        "\n",
        "    # add link to header of response\n",
        "    headers['referer'] = f'{base_referer}{page_number}'\n",
        "    params['page'] = f'{page_number}'\n",
        "\n",
        "    response = requests.get('https://www.screener.in/screen/raw/', params=params, cookies=cookies, headers=headers).text\n",
        "\n",
        "    soup = BeautifulSoup(response,'lxml')\n",
        "    webpage = soup.find_all('a',attrs={\"target\":\"_blank\"})\n",
        "\n",
        "    for item in webpage:\n",
        "      link = item.attrs.get('href')\n",
        "      links.append(link)\n"
      ],
      "metadata": {
        "id": "9mtpYp-X9Qtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps to fetch details of all comanpy and store in list"
      ],
      "metadata": {
        "id": "Gbxoi4gn9th4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cookies = {\n",
        "    'csrftoken': 'G7OthSSwzawWdx0L4I6p8IMaaHCBPw7r',\n",
        "    'sessionid': '9dchbljukn6ogjc4bu2utbpsgj4k5ybf',\n",
        "    'theme': 'light',\n",
        "}\n",
        "\n",
        "quick_api_cookies = {\n",
        "    'csrftoken': 'G7OthSSwzawWdx0L4I6p8IMaaHCBPw7r',\n",
        "    'sessionid': '9dchbljukn6ogjc4bu2utbpsgj4k5ybf',\n",
        "    'theme': 'light',\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'authority': 'www.screener.in',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'cache-control': 'max-age=0',\n",
        "    # 'cookie': 'csrftoken=G7OthSSwzawWdx0L4I6p8IMaaHCBPw7r; sessionid=9dchbljukn6ogjc4bu2utbpsgj4k5ybf; theme=light',\n",
        "    'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
        "    'sec-ch-ua-mobile': '?1',\n",
        "    'sec-ch-ua-platform': '\"Android\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',\n",
        "}\n",
        "\n",
        "quick_api_headers = {\n",
        "    'authority': 'www.screener.in',\n",
        "    'accept': '*/*',\n",
        "    'accept-language': 'en-US,en;q=0.9,mr;q=0.8',\n",
        "    # 'cookie': 'csrftoken=G7OthSSwzawWdx0L4I6p8IMaaHCBPw7r; sessionid=9dchbljukn6ogjc4bu2utbpsgj4k5ybf; theme=light',\n",
        "    'referer': 'https://www.screener.in/company/526301/consolidated/',\n",
        "    'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
        "    'sec-ch-ua-mobile': '?1',\n",
        "    'sec-ch-ua-platform': '\"Android\"',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'sec-fetch-mode': 'cors',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',\n",
        "    'x-requested-with': 'XMLHttpRequest',\n",
        "}\n"
      ],
      "metadata": {
        "id": "3wOPy8XnjJI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# links1 = links[12:]\n",
        "# links2 = links1[21:]\n",
        "# links3 = links2[2485:]\n",
        "# links4 = links[0:1]\n",
        "# links5 = links[0:1]\n",
        "len(links)"
      ],
      "metadata": {
        "id": "O_xDUqGSxK5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Sign_Remover(fun_data):\n",
        "\n",
        "  fun_data.index = fun_data.index.str.replace('+', '',regex=False)\n",
        "\n",
        "  json_data = fun_data.to_json()\n",
        "\n",
        "  # Remove text after the double backslash\n",
        "  json_data_cleaned = re.sub(r'\\\\u00a0\\.*', '', json_data)\n",
        "\n",
        "  # Load the cleaned JSON data\n",
        "  # data = json.loads(json_data_cleaned)\n",
        "\n",
        "  df2 = pd.read_json(json_data_cleaned)\n",
        "\n",
        "  return df2"
      ],
      "metadata": {
        "id": "p7e-YOSp61z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BSE_NSE():\n",
        "  # Find all span elements with class \"ink-700 upper\" using BeautifulSoup\n",
        "  BSE_NSE_value = soup.find_all(\"span\", attrs={\"class\": \"ink-700 upper\"})\n",
        "\n",
        "  # Initialize variables\n",
        "  value_BSE = BSE_NSE_value[0].text.strip().find(\"BSE\")  # Check for \"BSE\" in the first span text\n",
        "  value_NSE = -1\n",
        "\n",
        "  # # Check if there are more than 2 elements in BSE_NSE_value\n",
        "  if len(BSE_NSE_value) > 2:\n",
        "      # Check for \"NSE\" in the text of the second span\n",
        "      value_NSE = BSE_NSE_value[1].text.find(\"NSE\")\n",
        "\n",
        "  # # Check if \"BSE\" is found in the first span\n",
        "  if not value_BSE == -1 :\n",
        "      # Extract BSE information by splitting the text and stripping whitespace\n",
        "      Company_Basic_Info[\"BSE\"] = BSE_NSE_value[0].text.split(\":\")[-1].strip()\n",
        "\n",
        "      # Check if \"NSE\" is found in the second span\n",
        "      if not value_NSE == -1:\n",
        "          # Extract NSE information by splitting the text and stripping whitespace\n",
        "          Company_Basic_Info[\"NSE\"] = BSE_NSE_value[1].text.split(\":\")[-1].strip()\n",
        "      else:\n",
        "        Company_Basic_Info[\"NSE\"] = \"\"\n",
        "  else:\n",
        "      # If \"BSE\" is not found, extract NSE information from the first span\n",
        "      Company_Basic_Info[\"NSE\"] = BSE_NSE_value[0].text.split(\":\")[-1].strip()\n",
        "      Company_Basic_Info[\"BSE\"] = \"\""
      ],
      "metadata": {
        "id": "FWBxglgf1LX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Save():\n",
        "  path =  f'/content/drive/MyDrive/Screener_Comapany_Data/{Company_Basic_Info[\"Company_name\"]}'\n",
        "\n",
        "  isExist = os.path.exists(path)\n",
        "\n",
        "  if not isExist:\n",
        "    os.mkdir(path)\n",
        "\n",
        "  # Convert the dictionary into a DataFrame\n",
        "  df = pd.DataFrame([Company_Basic_Info])\n",
        "\n",
        "  # Save the Data to a CSV file\n",
        "  df.to_csv(f'{path}/{Company_Basic_Info[\"Company_name\"]}_Basic_Info.csv')\n",
        "\n",
        "  Quarterly_Profit_Loss.to_csv(f'{path}/Quarterly_Profit_Loss.csv')\n",
        "\n",
        "  Yearly_Profit_Loss.to_csv(f'{path}/Yearly_Profit_Loss.csv')\n",
        "\n",
        "  Yearly_Balance_Sheet.to_csv(f'{path}/Yearly_Balance_Sheet.csv')\n",
        "\n",
        "  Yearly_Cash_flow.to_csv(f'{path}/Yearly_Cash_flow.csv')\n",
        "\n",
        "  Ratios.to_csv(f'{path}/Ratios.csv')\n",
        "\n",
        "  if len(all_table) > 10:\n",
        "    Quarterly_Shareholding_Pattern.to_csv(f'{path}/Quarterly_Shareholding_Pattern.csv')\n",
        "\n",
        "    Yearly_Shareholding_Pattern.to_csv(f'{path}/Yearly_Shareholding_Pattern.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "E4EO61WkVC0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "for link in links:\n",
        "\n",
        "  # debugging\n",
        "  count += 1\n",
        "  print(f'the count is {count} and company link is https://www.screener.in{link}')\n",
        "\n",
        "  response = requests.get(f'https://www.screener.in{link}', cookies=cookies, headers=headers)\n",
        "\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "\n",
        "\n",
        "  '''\n",
        "  ===============================================================\n",
        "  ========================= All Table ===========================\n",
        "  ===============================================================\n",
        "  '''\n",
        "  all_table = pd.read_html(str(soup), index_col=0)\n",
        "\n",
        "  Quarterly_Profit_Loss = all_table[0].drop(\"Raw PDF\", axis='rows')\n",
        "  Quarterly_Profit_Loss = Sign_Remover(Quarterly_Profit_Loss)\n",
        "\n",
        "  Yearly_Profit_Loss = all_table[1].head(-1)\n",
        "  Yearly_Profit_Loss = Sign_Remover(Yearly_Profit_Loss)\n",
        "\n",
        "  Yearly_Balance_Sheet = all_table[6]\n",
        "  Yearly_Balance_Sheet = Sign_Remover(Yearly_Balance_Sheet)\n",
        "\n",
        "  Yearly_Cash_flow = all_table[7]\n",
        "  Yearly_Cash_flow = Sign_Remover(Yearly_Cash_flow)\n",
        "\n",
        "  Ratios = all_table[8]\n",
        "  Ratios = Sign_Remover(Ratios)\n",
        "\n",
        "  if len(all_table) > 10:\n",
        "\n",
        "    Quarterly_Shareholding_Pattern = all_table[9]\n",
        "    Quarterly_Shareholding_Pattern = Sign_Remover(Quarterly_Shareholding_Pattern)\n",
        "\n",
        "    Yearly_Shareholding_Pattern = all_table[10]\n",
        "    Yearly_Shareholding_Pattern = Sign_Remover(Yearly_Shareholding_Pattern)\n",
        "\n",
        "\n",
        "  '''\n",
        "  ===============================================================\n",
        "  ===================== Company Basic Info ======================\n",
        "  ===============================================================\n",
        "  '''\n",
        "  Company_Basic_Info = {}\n",
        "\n",
        "  Company_Basic_Info[\"Company_name\"] = soup.find(\"h1\",attrs={'class':'margin-0 show-from-tablet-landscape'}).text\n",
        "  Company_Basic_Info[\"Sector\"] = soup.find_all(\"p\",attrs={'class':'sub'})[1].a.text.replace('\\n','').strip()\n",
        "\n",
        "  BSE_NSE()\n",
        "\n",
        "\n",
        "\n",
        "  '''\n",
        "  ===============================================================\n",
        "  ========================= ALL Ratios ==========================\n",
        "  ===============================================================\n",
        "  '''\n",
        "\n",
        "\n",
        "  # =========== ALL default Ratios ===========\n",
        "\n",
        "  default_ratios = soup.find_all(\"li\", attrs={\"class\":\"flex flex-space-between\", \"data-source\":\"default\"})\n",
        "\n",
        "  for ratio in default_ratios:\n",
        "    ratio_arr = str(ratio.text).replace(\"\\n\", \"\").replace(\"         Cr.\",\"Cr.\").replace(\"         %\",\"%\").replace(\"₹       \", \"₹\").strip().split(\"      \")\n",
        "    last_value_check = ratio_arr[-1].strip().replace(\"Cr.\",\"\").replace(\"%\",\"\").replace(\"₹\",\"\").strip()\n",
        "\n",
        "    if len(ratio_arr) > 1:\n",
        "      if len(last_value_check) >= 2:\n",
        "        Company_Basic_Info[f\"{ratio_arr[0]}\"] = ratio_arr[-1].strip()\n",
        "      else:\n",
        "        Company_Basic_Info[f\"{ratio_arr[0]}\"] = \"\"\n",
        "    else:\n",
        "      Company_Basic_Info[f\"{ratio_arr[0]}\"] = \"\"\n",
        "\n",
        "\n",
        "  # =========== ALL API Ratios ===========\n",
        "\n",
        "  # company id is used for to fetch more ratios\n",
        "  Company_Id = soup.find(\"div\", attrs = {\"id\":\"company-info\"}).attrs.get(\"data-warehouse-id\")\n",
        "\n",
        "  # this is used for to fetch ratios\n",
        "  quick_api_response = requests.get(f'https://www.screener.in/api/company/{Company_Id}/quick_ratios/', cookies=quick_api_cookies, headers=quick_api_headers)\n",
        "  quick_ratios_soup = BeautifulSoup(quick_api_response.content,'html.parser')\n",
        "\n",
        "  quick_api_ratios = quick_ratios_soup.find_all(\"li\")\n",
        "\n",
        "  for api_ratios in quick_api_ratios:\n",
        "    api_ratio_arr = str(api_ratios.text).replace(\"\\n\", \"\").replace(\"         Cr.\",\"Cr.\").replace(\"         %\",\"%\").replace(\"₹       \", \"₹\").strip().split(\"      \")\n",
        "\n",
        "\n",
        "    Field_name = api_ratio_arr[0].strip().replace(\"Var\",\"growth\").replace(\"Bk\",\"back\").replace(\"Yrs\",\"Years\")\n",
        "\n",
        "    if len(api_ratio_arr) >= 2:\n",
        "      last_value_check = api_ratio_arr[-1].strip().replace(\"Cr.\",\"\").replace(\"%\",\"\").replace(\"₹\",\"\").strip()\n",
        "\n",
        "      if len(last_value_check) > 2 :\n",
        "        Company_Basic_Info[f\"{Field_name}\"] = api_ratio_arr[-1].strip()\n",
        "      else:\n",
        "        Company_Basic_Info[f\"{Field_name}\"] = \"\"\n",
        "    else:\n",
        "      Company_Basic_Info[f\"{Field_name}\"] = \"\"\n",
        "\n",
        "\n",
        "  Save()\n"
      ],
      "metadata": {
        "id": "626k5P_o_H-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}